# -*- coding: utf-8 -*-
"""seq2seq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L82mH2-TNLjDAwXGX1I1XgEdsojGUsRg
"""

import json
from google.colab import drive
drive.mount('/content/drive')

project_dir = "/content/drive/My Drive/QA/"

# Load the data
f = open(project_dir+"train-v2.0.json",mode="r",encoding="utf-8")
raw_data = json.load(f)

pip install transformers -U

# Pre-process the data
import nltk
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
nltk.download("punkt")

def pre_process(raw_data,max_input_samples,max_context_length,max_question_length,max_answer_length):
  """keys = {
        "version":
        "data":[{
          "title":
          "paragraphs":[{
                  "qas":[{"question":
              "answers":
              }]
            "context":
          }]

        }]
             }  """
  "Fill here"
  data = []
  data_tokenized = []
  data1 = raw_data["data"]
  count = 0
  contexts = []
  questions = []
  answers = []
  for data_ele in data1:
    for para in data_ele["paragraphs"]:
      qas = para["qas"]
      context = [k.lower() for k in nltk.word_tokenize(para["context"])]
      #context_tokenized = tokenizer(para["context"], padding = True)
      if len(context) <= max_context_length:
        for qa in qas:
          que = [k.lower() for k in nltk.word_tokenize(qa["question"])]
          #que_tokenized = tokenizer(qa["question"], padding = True)
          if len(qa["answers"]) == 1:
            ans = [k.lower() for k in nltk.word_tokenize(qa["answers"][0]["text"])]
            ans_start = qa["answers"][0]["answer_start"]
            answer_indices = [ans_start, ans_start+len(ans)]
            if "how" not in que and "why" not in que:
              if len(que) <= max_question_length and len(que) <= max_answer_length:
                if count <= max_input_samples:
                  data.append([context,que,ans])
                  contexts.append(para["context"])
                  questions.append(qa["question"])
                  #data_tokenized.append([context_tokenized,que_tokenized,answer_indices])
                  count = count + 1
                else:
                  break
    if count >  max_input_samples:
      break
  return data, contexts, questions, answer_indices

max_input_samples = 1000
max_context_length = 300
max_question_length = 50
max_answer_length = 50

data, contexts, questions, answer_indices = pre_process(raw_data,max_input_samples,max_context_length,max_question_length,max_answer_length)
contexts_tokenized = tokenizer(contexts, padding = True, truncation= True, return_tensors = "pt")
questions_tokenized = tokenizer(questions, padding= True, truncation = True, return_tensors = "pt")

contexts_tokenized = contexts_tokenized["input_ids"]
questions_tokenized = questions_tokenized["input_ids"]

## data distribution
what_count = 0
who_count = 0
when_count = 0
where_count = 0
ques_length_list = []
for one_set in data:
  ques_length = 0
  for ques_word in one_set[1]:
    ques_length = ques_length + 1
    if ques_word == 'when':
      when_count = when_count + 1
    elif ques_word == 'what':
      what_count = what_count + 1
    elif ques_word == 'who':
      who_count = who_count + 1
    elif ques_word == 'where':
      where_count = where_count + 1
  ques_length_list.append(ques_length)
#print(when_count)
#print(who_count)
#print(what_count)
#print(where_count)
#print(ques_length_list)

ques_data = {}
ques_data['when'] = when_count
ques_data['what'] = what_count
ques_data['who'] = who_count
ques_data['where'] = where_count
names = list(ques_data.keys())
values = list(ques_data.values())

from matplotlib import pyplot as plt

fig, axs = plt.subplots(1, 2, figsize=(12, 7))
axs[0].bar(names, values)
axs[1].hist(ques_length_list, bins = [i*5 for i in range(11)])

from collections import Counter
import numpy as np
from sklearn.model_selection import train_test_split
import nltk

WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'


def in_white_list(_word):
    valid_word = False
    for char in _word:
        if char in WHITELIST:
            valid_word = True
            break
    if valid_word is False:
        return False
    return True

max_target_vocab_size = 5000
max_input_vocab_size = 5000

data_set = raw_data["data"]
input_data_samples = []
output_data_samples = []

input_max_seq_length = 0
target_max_seq_length = 0

input_counter = Counter()
target_counter = Counter()

input_data_samples = []
output_data_samples = []

data_set1 = []
count = 0
for sample in data_set:
  for para in sample["paragraphs"]:
      qas = para["qas"]
      context = [k.lower() for k in nltk.word_tokenize(para["context"])]
      if len(context) <= max_context_length:
        for qa in qas:
          que = [k.lower() for k in nltk.word_tokenize(qa["question"])]
          if len(qa["answers"]) == 1:
            if "how" not in que and "why" not in que:
              if len(que) <= max_question_length and len(que) <= max_answer_length:
                if count <= max_input_samples:
                  data_set1.append([para["context"],qa["question"],qa["answers"][0]["text"]])
                  count = count + 1
                else:
                  break
  if count > max_input_samples:
    break

for sample in data_set1:
    #print(sample)
    paragraph, question, answer = sample
    paragraph_word_list = [w.lower() for w in nltk.word_tokenize(paragraph) if in_white_list(w)]
    question_word_list = [w.lower() for w in nltk.word_tokenize(question) if in_white_list(w)]
    answer_word_list = [w.lower() for w in nltk.word_tokenize(answer) if in_white_list(w)]

    input_data = paragraph_word_list + ['Q'] + question_word_list
    output_data = ['START'] + answer_word_list + ['END']

    input_data_samples.append(input_data)
    output_data_samples.append(output_data)

    for w in input_data:
        input_counter[w] += 1
    for w in output_data:
        target_counter[w] += 1

    input_max_seq_length = max(input_max_seq_length, len(input_data))
    target_max_seq_length = max(target_max_seq_length, len(output_data))

input_word2idx = dict()
target_word2idx = dict()
for idx, word in enumerate(input_counter.most_common(max_input_vocab_size)):
    input_word2idx[word[0]] = idx + 2
for idx, word in enumerate(target_counter.most_common(max_target_vocab_size)):
    target_word2idx[word[0]] = idx + 1

target_word2idx['UNK'] = 0
input_word2idx['PAD'] = 0
input_word2idx['UNK'] = 1

input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])
target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])

num_input_tokens = len(input_idx2word)
num_target_tokens = len(target_idx2word)

input_encoded_data_samples = []
target_encoded_data_samples = []

for input_data, output_data in zip(input_data_samples, output_data_samples):
    input_encoded_data = []
    target_encoded_data = []
    for word in input_data:
        if word in input_word2idx:
            input_encoded_data.append(input_word2idx[word])
        else:
            input_encoded_data.append(1)
    for word in output_data:
        if word in target_word2idx:
            target_encoded_data.append(target_word2idx[word])
        else:
            target_encoded_data.append(0)
    input_encoded_data_samples.append(input_encoded_data)
    target_encoded_data_samples.append(target_encoded_data)

samples = [input_encoded_data_samples, target_encoded_data_samples]
print(samples[0][0])

print(input_max_seq_length)

def generate_batch(input_data, target_data, batch_size):
    num_batches = len(input_data) // batch_size

    while True:
        for batchIdx in range(0, num_batches):
            start = batchIdx * batch_size
            end = (batchIdx + 1) * batch_size
            encoder_input_data_batch = pad_sequences(input_data[start:end], input_max_seq_length)
            decoder_target_data_batch = np.zeros(shape=(batch_size, target_max_seq_length,
                                                        num_target_tokens))
            decoder_input_data_batch = np.zeros(shape=(batch_size, target_max_seq_length,
                                                       num_target_tokens))
            for lineIdx, target_wid_list in enumerate(target_data[start:end]):
                for idx, wid in enumerate(target_wid_list):
                    if wid == 0:  # UNKNOWN
                        continue
                    decoder_input_data_batch[lineIdx, idx, wid] = 1
                    if idx > 0:
                        decoder_target_data_batch[lineIdx, idx - 1, wid] = 1
            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch

# Create a model

from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau
from keras.models import Model
from keras.layers import Input, LSTM, Dense, Embedding, Dropout, add, RepeatVector
from tensorflow.keras.optimizers import SGD

def create_model(num_input_tokens, input_max_seq_length, num_target_tokens):
        hidden_units = 256

        encoder_inputs = Input(shape=(None,), name='encoder_inputs')
        encoder_embedding = Embedding(input_dim=num_input_tokens, output_dim=hidden_units,
                                      input_length=input_max_seq_length, name='encoder_embedding')

        encoder_lstm = LSTM(units=hidden_units, return_state=True, name='encoder_lstm')
        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))
        encoder_states = [encoder_state_h, encoder_state_c]

        decoder_inputs = Input(shape=(None, num_target_tokens), name='decoder_inputs')
        decoder_lstm = LSTM(units=hidden_units, return_state=True, return_sequences=True, name='decoder_lstm')
        decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,
                                                                         initial_state=encoder_states)
        decoder_dense = Dense(units=num_target_tokens, activation='softmax', name='decoder_dense')
        decoder_outputs = Dropout(0.3)(decoder_outputs)
        decoder_outputs = decoder_dense(decoder_outputs)

        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
        #opt = SGD(lr=0.01)
        model.compile(loss='categorical_crossentropy', optimizer="rmsprop", metrics=['accuracy'])

        encoder_model = Model(encoder_inputs, encoder_states)

        decoder_state_inputs = [Input(shape=(hidden_units,)), Input(shape=(hidden_units,))]
        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)
        decoder_states = [state_h, state_c]
        decoder_outputs = decoder_dense(decoder_outputs)
        decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)

        return model, encoder_model, decoder_model

model, encoder_model, decoder_model = create_model(num_input_tokens, input_max_seq_length, num_target_tokens)
model.summary()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(samples[0], samples[1], test_size=0.2,random_state=40)

from keras.preprocessing.sequence import pad_sequences

batch_size = 64
train_gen = generate_batch(x_train, y_train, batch_size)
test_gen = generate_batch(x_test, y_test, batch_size)
a = next(train_gen)
a = next(train_gen)

b = a[0][0]
r = (len(b),len(b[0]))
print(r)
train_num_batches = len(x_train) // batch_size
test_num_batches = len(x_test) // batch_size

weight_file_path = project_dir + "seq2seq200.h5"
checkpoint = ModelCheckpoint(filepath=weight_file_path, monitor='val_loss', mode='min', save_best_only=True,verbose=1)
reduce_alpha = ReduceLROnPlateau(monitor ='val_loss', factor = 0.2, patience = 1, min_lr = 0.001)
#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=25)

history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,
                                    epochs=200,
                                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches,
                                    callbacks=[checkpoint,reduce_alpha])

model.save_weights(weight_file_path)

# Test the model

def reply(paragraph, question):
        input_seq = []
        input_wids = []
        input_text = paragraph.lower() + ' Q ' + question.lower()
        for word in nltk.word_tokenize(input_text):
            if word != 'Q' and (not in_white_list(word)):
                continue
            idx = 1  # default [UNK]
            if word in input_word2idx:
                idx = input_word2idx[word]
            input_wids.append(idx)
        input_seq.append(input_wids)
        input_seq = pad_sequences(input_seq, input_max_seq_length)
        states_value = encoder_model.predict(input_seq)
        target_seq = np.zeros((1, 1, num_target_tokens))
        target_seq[0, 0, target_word2idx['START']] = 1
        target_text = ''
        target_text_len = 0
        terminated = False
        while not terminated:
            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

            sample_token_idx = np.argmax(output_tokens[0, -1, :])
            sample_word = target_idx2word[sample_token_idx]
            target_text_len += 1

            if sample_word != 'START' and sample_word != 'END':
                target_text += ' ' + sample_word

            if sample_word == 'END' or target_text_len >= target_max_seq_length:
                terminated = True

            target_seq = np.zeros((1, 1, num_target_tokens))
            target_seq[0, 0, sample_token_idx] = 1

            states_value = [h, c]
        return target_text.strip()


for i in range(20):
    index = i * 10
    paragraph, question, actual_answer = data_set1[index]
    predicted_answer = reply(paragraph, question)
    print('context: ', paragraph)
    print('question: ', question)
    print({'guessed_answer': predicted_answer, 'actual_answer': actual_answer})
    print("\n")

# precision, recall, f1-score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
precision = precision_score(true_values, predict_values)
recall = recall_score(true_values, predict_values)
f1score = f1_score(true_values, predict_values)

